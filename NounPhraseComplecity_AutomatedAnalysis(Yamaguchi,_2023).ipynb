{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUaM_-GlS7gs"
      },
      "outputs": [],
      "source": [
        "!pip install stanza\n",
        "import stanza\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency,mwt,lemma,depparse')\n",
        "\n",
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import glob\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Analysing a single sentence**"
      ],
      "metadata": {
        "id": "jqtWi7uTTzdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depDic={\"compound\":\"com\", \"acl:relcl\":\"rel\", \"nmod\":\"prep\", \"amod\":\"adj\", \"det\":\"det\", \"nmod:poss\":\"det\", \"nummod\":\"nummod\"}\n",
        "\n",
        "doc = nlp(\"the significance of having a part-time job for Tom is quite clear.\") # Chane a sentence here\n",
        "deprel_list=[]\n",
        "\n",
        "\n",
        "for sentence in doc.sentences:\n",
        "\n",
        "#### Process related to noun phrases ####\n",
        "\n",
        "  # Convert constituency parsing into a tree object which is handlable in nltk toolkit\n",
        "    tree = Tree.fromstring(str(sentence.constituency))\n",
        "\n",
        "  # extract NP subtrees\n",
        "    nps = [subtree for subtree in tree.subtrees() if subtree.label() == \"NP\"]\n",
        "\n",
        "  # Display the surface forms of the largest NP subtrees that do not have \"PRP (pronoun)\", \"EX (it/there)\", or \"NNP (proper noun)\" as their head.\n",
        "    NP_list=[]\n",
        "    NGNP_list=[\"It\", \"There\",\"it\", \"there\" ]\n",
        "    for np in nps:\n",
        "      if any(subtree.label() not in  [\"PRP\",\"EX\", \"NNP\",\"DT\"] for subtree in np):\n",
        "        surface_form = \" \".join(np.leaves())\n",
        "        NP_list.append(surface_form)\n",
        "\n",
        "    for NP1 in NP_list: # Remove overlaps unique to constituency parsing (check for the presence of substring matches at the string level and remove those that are partially matched)\n",
        "      for NP2 in NP_list:\n",
        "        if NP1 == NP2:\n",
        "          continue\n",
        "        elif NP1 in NP2:\n",
        "          NGNP_list.append(NP1)\n",
        "\n",
        "    NP_list = [x for x in NP_list if x not in NGNP_list]\n",
        "    print(\"extracted NPs:\", NP_list)\n",
        "\n",
        "\n",
        "#### Process related to noun modifiers ####\n",
        "    deprel_list=[]\n",
        "    for word in  sentence.words:\n",
        "\n",
        "      if word.deprel in [\"compound\",\"acl:relcl\",\"nmod\",\"amod\",\"acl\", \"det\", \"nmod:poss\",\"nummod\"]:  #　Extract words with tags matching　\"compound\",\"acl:relcl\",\"nmod\",\"amod\",\"acl\", \"det\", \"nmod:poss\",\"nummod\"\n",
        "\n",
        "        if word.deprel == \"acl\":\n",
        "\n",
        "          if word.xpos ==\"VBG\": # exception handling related to \"prep VBG (e.g., significance of -ing)\"\n",
        "              head_word_id=word.id\n",
        "              dep_list=[]\n",
        "              for word_2 in sentence.words:\n",
        "                  if word_2.head == head_word_id:\n",
        "                    dep_list.append(word_2.xpos)\n",
        "              if \"IN\" in dep_list:\n",
        "                deprel_list.append(\"prep\")\n",
        "              else:\n",
        "                deprel_list.append(\"ing-acl\")\n",
        "\n",
        "          if word.xpos ==\"VBN\":  # exception handling related to \"to be VBN (e.g.,things to be done)\"\n",
        "              head_word_id=word.id\n",
        "              dep_list=[]\n",
        "              for word_2 in sentence.words:\n",
        "                  if word_2.head == head_word_id:\n",
        "                    dep_list.append(word_2.xpos)\n",
        "              if \"TO\" and \"VB\" in dep_list:\n",
        "                deprel_list.append(\"to-acl\")\n",
        "\n",
        "              else:\n",
        "                deprel_list.append(\"ed-acl\")\n",
        "\n",
        "          if word.xpos ==\"VB\":\n",
        "            deprel_list.append(\"to-acl\")\n",
        "\n",
        "        else:\n",
        "          deprel_list.append(depDic[word.deprel])\n",
        "\n",
        "    print(\"extracted noun modifiers:\",deprel_list )\n",
        "\n",
        "# if you want to see the dependency parsing out put, delele #\n",
        "# print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
        "\n",
        "# if you want to see the constituency parsing out put, delele #\n",
        "# tree.pretty_print()"
      ],
      "metadata": {
        "id": "1KG5di-weGMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysing multiple files in a directory**"
      ],
      "metadata": {
        "id": "IZ2vw1zbR6_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depDic={\"compound\":\"com\", \"acl:relcl\":\"rel\", \"nmod\":\"prep\", \"amod\":\"adj\", \"det\":\"det\", \"nmod:poss\":\"det\", \"nummod\":\"nummod\"}\n",
        "df_deprel = pd.DataFrame(columns=['filename','sentenceID', 'sentence', \"deprel\"])\n",
        "df_NP = pd.DataFrame(columns=['filename','sentenceID','sentence', 'NP'])\n",
        "npID=0\n",
        "depID=0\n",
        "\n",
        "files = glob.glob('/content/drive/MyDrive/2023_JUNE_AutoAnalysis/NS/*.txt')  # Chane the path here\n",
        "for i in files:\n",
        "  filename = pathlib.PurePath(i).stem\n",
        "  with open(i, 'r') as file:\n",
        "      text = file.read()\n",
        "\n",
        "  doc = nlp(text)\n",
        "\n",
        "  sentenceID=0\n",
        "  for sentence in doc.sentences:\n",
        "    sentenceID=sentenceID+1\n",
        "#### Process related to noun phrases ####\n",
        "    tree = Tree.fromstring(str(sentence.constituency))\n",
        "    nps = [subtree for subtree in tree.subtrees() if subtree.label() == \"NP\"]\n",
        "    NP_list=[]\n",
        "    NGNP_list=[\"It\", \"There\",\"it\", \"there\" ]\n",
        "    for np in nps:\n",
        "      if any(subtree.label() not in  [\"PRP\",\"EX\", \"NNP\",\"DT\"] for subtree in np):\n",
        "        surface_form = \" \".join(np.leaves())\n",
        "        NP_list.append(surface_form)\n",
        "\n",
        "    for NP1 in NP_list:\n",
        "      for NP2 in NP_list:\n",
        "        if NP1 == NP2:\n",
        "          continue\n",
        "        elif NP1 in NP2:\n",
        "          NGNP_list.append(NP1)\n",
        "\n",
        "    NP_list = [x for x in NP_list if x not in NGNP_list]\n",
        "\n",
        "    for np in NP_list:\n",
        "      npID=npID+1\n",
        "      df_NP.loc[npID, 'filename'] = filename\n",
        "      df_NP.loc[npID, 'NP'] = np\n",
        "      df_NP.loc[npID, 'sentenceID']= sentenceID\n",
        "      df_NP.loc[npID, 'sentence']= sentence.text\n",
        "\n",
        "#### Process related to noun modifiers ####\n",
        "    deprel_list=[]\n",
        "    for word in  sentence.words:\n",
        "      if word.deprel in [\"compound\",\"acl:relcl\",\"nmod\",\"amod\",\"acl\", \"det\", \"nmod:poss\",\"nummod\"]:  #　Extract words with tags matching　\"compound\",\"acl:relcl\",\"nmod\",\"amod\",\"acl\", \"det\", \"nmod:poss\",\"nummod\"\n",
        "\n",
        "        if word.deprel == \"acl\":\n",
        "          if word.xpos ==\"VBG\": # exception handling related to \"prep VBG (e.g., significance of -ing)\"\n",
        "              head_word_id=word.id\n",
        "              dep_list=[]\n",
        "              for word_2 in sentence.words:\n",
        "                  if word_2.head == head_word_id:\n",
        "                    dep_list.append(word_2.xpos)\n",
        "              if \"IN\" in dep_list:\n",
        "                deprel_list.append(\"prep\")\n",
        "              else:\n",
        "                deprel_list.append(\"ing-acl\")\n",
        "\n",
        "          if word.xpos ==\"VBN\":  # exception handling related to \"to be VBN (e.g.,things to be done)\"\n",
        "              head_word_id=word.id\n",
        "              dep_list=[]\n",
        "              for word_2 in sentence.words:\n",
        "                  if word_2.head == head_word_id:\n",
        "                    dep_list.append(word_2.xpos)\n",
        "              if \"TO\" and \"VB\" in dep_list:\n",
        "                deprel_list.append(\"to-acl\")\n",
        "\n",
        "              else:\n",
        "                deprel_list.append(\"ed-acl\")\n",
        "\n",
        "          if word.xpos ==\"VB\":\n",
        "            print(word, \"to-acl\")\n",
        "\n",
        "        else:\n",
        "          deprel_list.append(depDic[word.deprel])\n",
        "\n",
        "    for dep in deprel_list:\n",
        "      depID=depID+1\n",
        "      df_deprel.loc[depID, 'filename'] = filename\n",
        "      df_deprel.loc[depID, 'deprel'] = dep\n",
        "      df_deprel.loc[depID, 'sentenceID']= sentenceID\n",
        "      df_deprel.loc[depID, 'sentence']= sentence.text\n"
      ],
      "metadata": {
        "id": "h7CG4sXrR51l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computing the value of indices**"
      ],
      "metadata": {
        "id": "yfBZXCHtSxQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pathlib\n",
        "from collections import Counter\n",
        "\n",
        "All_df=pd.DataFrame(columns=['filename',\"CEFR\",'mean_length_of_NPs','mean_number_of_modifiers', 'types_of_modifiers', 'det_frequency', 'adj_frequency',  'com_frequency', 'ed-acl_frequency', 'ing-acl_frequency','to-acl_frequency', 'rel_frequency','prep_frequency', 'nummod_frequency'])\n",
        "fileID=0\n",
        "modifierList=['det', 'prep', 'adj', 'ed-acl', 'ing-acl', 'rel', 'com', 'to-acl', 'nummod']\n",
        "\n",
        "files = glob.glob('/content/drive/MyDrive/2023_JUNE_AutoAnalysis/A2/*.txt') # Chane the path here\n",
        "for i in files:\n",
        "  fileID=fileID+1\n",
        "  filename = pathlib.PurePath(i).stem\n",
        "  All_df.loc[fileID, 'filename'] = filename\n",
        "\n",
        "#### Process related to Mean Length of NPs ####\n",
        "  list=[]\n",
        "  for index, row in df_NP[df_NP[\"filename\"]==filename].iterrows():\n",
        "      number_of_words = len(row[\"NP\"].split())\n",
        "      list.append(number_of_words)\n",
        "  mean_length_of_NPs = np.mean(list)\n",
        "  All_df.loc[fileID, 'mean_length_of_NPs'] = mean_length_of_NPs\n",
        "\n",
        "#### Process related to Mean Number of Modifiers ####\n",
        "  NP_condition = df_NP[\"filename\"]==filename\n",
        "  NP_count = (NP_condition).sum()\n",
        "  dep_condition = df_deprel[\"filename\"]==filename\n",
        "  dep_count = (dep_condition).sum()\n",
        "  mean_number_of_modifiers=dep_count/NP_count\n",
        "  All_df.loc[fileID, 'mean_number_of_modifiers'] = mean_number_of_modifiers\n",
        "\n",
        "#### Number of Modifier Types ####\n",
        "  dep_counter = Counter(df_deprel[df_deprel[\"filename\"]==filename]['deprel'])\n",
        "  dep_frequency_dict = dict(dep_counter)\n",
        "  types_of_modifiers= len(dep_frequency_dict)\n",
        "  All_df.loc[fileID, 'types_of_modifiers'] = types_of_modifiers\n",
        "\n",
        "#### Process related to Frequency of Each Modifier Type ####\n",
        "  for i in modifierList:\n",
        "    if i in dep_frequency_dict:\n",
        "      All_df.loc[fileID, i+\"_frequency\"] = dep_frequency_dict[i]/NP_count\n",
        "    else:\n",
        "      All_df.loc[fileID, i+\"_frequency\"] = 0\n",
        "\n",
        "csv_file_path = '2023_英語コーパス学会_分析用データ.csv' # Chane the path here\n",
        "All_df.to_csv(csv_file_path, index=False)"
      ],
      "metadata": {
        "id": "B05l5bXZTDNl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}